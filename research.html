<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Research</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <style>
    html { scroll-behavior: smooth; }

    /* TIMELINE STYLING */
    .timeline-container {
      overflow-x: auto;
      padding: 0.5rem 0;
      border-top: 1px solid #d1d5db;
      border-bottom: 1px solid #d1d5db;
      background-color: #f9fafb;
      scroll-behavior: smooth;
    }

    .timeline {
      position: relative;
      display: flex;
      align-items: center;
      justify-content: flex-start;
      gap: 2rem;
      width: max-content;
      min-width: 100%;
      margin: 0 auto;
      padding: 1.5rem 2rem;
    }

    .timeline::before {
      content: '';
      position: absolute;
      top: 50%;
      left: 0;
      width: 100%;
      height: 3px;
      background-color: #003366;
      transform: translateY(-50%);
      z-index: 0;
    }

    .timeline::after {
      content: '';
      position: absolute;
      top: 50%;
      right: -10px;
      width: 0;
      height: 0;
      border-top: 6px solid transparent;
      border-bottom: 6px solid transparent;
      border-left: 10px solid #003366;
      transform: translateY(-50%);
    }

    .timeline a {
      position: relative;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      text-align: center;
      color: #003366;
      font-weight: 600;
      padding: 0.5rem 0.75rem;
      z-index: 1;
      background-color: #fefefe;
      border-radius: 6px;
      transition: color 0.2s ease, transform 0.2s ease;
      white-space: nowrap;
    }

    .timeline a:hover {
      color: #2563eb;
      transform: translateY(-4px);
    }
  </style>
</head>

<body class="bg-[#fefefe] text-gray-800 font-sans">

  <!-- NAVIGATION BAR -->
  <nav class="bg-[#003366] text-white px-8 py-4 flex justify-between items-center sticky top-0 shadow-lg z-10">
  <!-- LEFT SIDE -->
  <div class="text-lg font-semibold tracking-wide">
    Ananya & Anisha
  </div>

  <!-- RIGHT SIDE LINKS -->
  <div class="flex gap-10 text-lg font-semibold">
  <a href="index.html" class="transition-transform hover:scale-110 hover:text-yellow-300">Home</a>
  <a href="research.html" class="transition-transform hover:scale-110 hover:text-yellow-300">Research</a>
  <a href="music.html" class="transition-transform hover:scale-110 hover:text-yellow-300">Music</a>
  <a href="teaching.html" class="transition-transform hover:scale-110 hover:text-yellow-300">Teaching</a>
  <a href="hiking.html" class="transition-transform hover:scale-110 hover:text-yellow-300">Hiking</a>
</div>
</nav>


  <!-- PAGE TITLE -->
  <div class="text-center mt-16 mb-10">
    <h1 class="text-4xl font-bold text-[#003366]">Research</h1>
  </div>

  <!-- CONNECTED TIMELINE -->
  <div class="timeline-container">
    <div class="timeline">
      <a href="#qtim">Sept 2025<br>QTIM Lab</a>
        <a href="#assip">July 2025<br>ASSIP - Anisha</a>
      <a href="#beaverworks">Aug 2025<br>MIT Beaverworks</a>
      <a href="#stanfordaimi">Jun 2025<br>Stanford AIMI</a>
      <a href="#insight">Jun 2025<br>InSight</a>
      <a href="#metafusion">Apr 2025<br>MetaFusion</a>
      <a href="#genelab">July 2024<br>NASA GeneLab - Ananya</a>
      <a href="#mediseeker">2024<br>Mediseeker</a>
      <a href="#tissueoforigin">2023<br>Tissue of Origin</a>
      <a href="#lendmeyourear">2023<br>Lend me your ear</a>
      <a href="#connectfour">2022<br>Connect Four</a>
    </div>
  </div>


  <!-- PROJECT SECTIONS -->
  <div class="max-w-4xl mx-auto px-6 py-10 space-y-20">

    <!-- QTIM LAB -->
    <section id="qtim">
      <h2 class="text-2xl font-bold text-[#003366] mb-2">QTIM Lab Intern (Sept 2025)</h2>
      <ul class="list-disc list-inside space-y-2">
        <li>Research Intern at Quantitative Translational Imaging in Medicine Lab @ CU Denver</li>
        <li><strong> AI-based diabetes progression prediction based on continuous glucose monitoring data from wearable sensors</strong>
          <ul class="list-disc list-inside ml-5">
            <li>Our work shows that compared to using HbA1c, which remains the standard for diagnosing diabetes, applying deep learning to continuous glucose monitoring data can provide more accurate detection of diabetes progression from healthy and prediabetic individuals to those managing the disease with oral medication.</li>
          </ul>
        </li>
        <div class="my-4">
          <!-- Replace below src with your actual image -->
          <img src="research_images/qtimproject1.png"
     alt="QTIM Project Image"
     class="rounded-lg shadow-md border border-gray-200 mx-auto w-3/4 md:w-2/3">

        </div>
  
      </ul>
    </section>

     <!-- ASSIP ANISHA -->
    <section id="assip">
      <h2 class="text-2xl font-bold text-[#003366] mb-2">ASSIP – Anisha (July 2025 – Present)</h2>
      <ul class="list-disc list-inside space-y-2">
        <li>Research intern at Aspiring Scientists Summer Internship Program, George Mason University, under Professor Mihai Boicu.</li>
        <li>Pulmonary embolism is a life-threatening condition with a high mortality rate that demands rapid diagnosis, relying on radiologists to manually review thousands of CTPA image slices.</li>
        <li>Developed a two-stage transformer-based pipeline that automates Pulmonary embolism detection using both CTPA imaging and electronic health records.</li>
        <li>The architecture improved accuracy by 3.1% compared to baseline models.</li>
        <li><strong>Accepted as a technical paper presentation at the IEEE MIT Undergraduate Research Technology Conference.</strong></li>

      </ul>
      <div class="my-4 flex justify-center">
  <img
    src="research_images/assip.png"
    alt="ASSIP Pulmonary Embolism Project"
    class="rounded-lg shadow-md border border-gray-200 w-full sm:w-[70%] md:w-[60%]"
  >
</div>
    </section>

   <!-- MIT BEAVERWORKS -->
<section id="beaverworks">
  <h2 class="text-2xl font-bold text-[#003366] mb-2">
    MIT Beaver Works Summer Institute Medlytics (July - October 2025)
  </h2>

  <ul class="list-disc list-inside space-y-2">
    <li>
      Selected for MIT BWSI Medlytics summer internship, where we learned about applications of AI and data analysis for medical data.
    </li>
    <li>
      Dysarthria is a motor speech disorder that results in slow and often incomprehensible speech.
      Speech intelligibility significantly impacts communication, leading to barriers in social interactions.
    </li>
    <li>
      We worked in a team of 4 to build a unified AI-based multilingual framework and web app to help dysarthric patients communicate effectively with dysarthria detection,
      severity classification, clean speech generation, speech-to-text conversion, and voice cloning.
    </li>
    <li>
      This work demonstrates a scalable, cross-lingual approach to diagnosing and supporting dysarthric patients.
    </li>
    <li>
      <strong>Accepted for poster presentation to Biomedical Engineering Society Annual Meeting 2025.</strong>
    </li>
    <li>
      <strong>Preprint:</strong>
      <a href="https://arxiv.org/abs/2510.03986"
         target="_blank"
         class="text-blue-600 hover:underline font-medium">
         arXiv: A Multilingual Framework for Dysarthria — Detection, Severity Classification, Speech-to-Text, and Clean Speech Generation
      </a>
    </li>
  </ul>

  <!-- Two poster images side by side -->
  <div class="flex flex-wrap justify-center items-center gap-4 mt-6">
    <img
      src="research_images/dysarthria1.png"
      alt="MIT Beaverworks Poster 1"
      class="rounded-lg shadow-md border border-gray-200 w-full sm:w-[48%] md:w-[45%]"
    >
    <img
      src="research_images/dysarthria2.png"
      alt="MIT Beaverworks Poster 2"
      class="rounded-lg shadow-md border border-gray-200 w-full sm:w-[48%] md:w-[45%]"
    >
  </div>
</section>


 
    <!-- STANFORD AIMI -->
<section id="stanfordaimi">
  <h2 class="text-2xl font-bold text-[#003366] mb-2">
    Stanford AIMI Research Interns (June 2025)
  </h2>

  <ul class="list-disc list-inside space-y-2">
    <li>
      Selected for Stanford Artificial Intelligence in Medical Imaging Summer Research Internship.
    </li>
    <li>
      Explored applications of AI for classification, segmentation, and localization of pneumonia from chest X-rays.
    </li>
    <li>
      At the final symposium, all projects were judged by Stanford faculty and our teams won first and second places.
    </li>
  </ul>

  <!-- Replace this placeholder once your poster image is ready -->
  <img src="research_images/aimi.png"
     alt="Stanford AIMI Poster"
     class="rounded-lg shadow-md border border-gray-200 mx-auto w-1/2 md:w-1/3">

</section>


 <!-- INSIGHT -->
<section id="insight">
  <h2 class="text-2xl font-bold text-[#003366] mb-2">
    InSight: AI-based Retinal Disease Screening Tool (June 2025)
  </h2>

  <ul class="list-disc list-inside space-y-2">
    <li>
      <strong>Developed InSight, an AI-based screening tool aimed at improving early detection of five major eye diseases:</strong>
      age-related macular degeneration, glaucoma, diabetic retinopathy, macular edema, and pathological myopia.
    </li>
    <li>
      These conditions collectively affect over 782 million people worldwide, yet access to screening remains limited in middle and low-income countries.
    </li>
    <li>
      Because many of these diseases are asymptomatic in their early stages, patients often miss the window for treatment. Currently, over 90% of the world’s blind population lives in lower-income regions.
    </li>
    <li>
      InSight integrates patient metadata with fundus images to enable accurate, real-time diagnosis through a three-stage pipeline:
      (1) Image Quality Assessment Model,
      (2) Multimodal Five-Disease Diagnosis Model,
      (3) Diabetic Retinopathy Severity Grading.
    </li>
    <li>
      The multitask model demonstrated strong generalizability across varied imaging conditions (lab-captured and mobile-captured fundus images) while detecting five diseases simultaneously.
    </li>
    <li>
      <strong>Accepted to two international conferences at SPIE Photonics West 2025:</strong>
      <ul class="list-disc list-inside ml-6">
        <li>
          <a href="https://spie.org/photonics-west/presentation/InSight--an-AI-based-screening-tool-for-eye-diseases/13839-15"
             target="_blank"
             class="text-blue-600 hover:underline font-medium">
             Ophthalmic Technologies Conference
          </a>
        </li>
        <li>
          <a href="https://spie.org/photonics-west/presentation/Applying-multimodal-fusion-and-pretraining-enhancements-to-improve-diagnostic-accuracy/13831-54"
             target="_blank"
             class="text-blue-600 hover:underline font-medium">
             Optics and Biophotonics in Low-Resource Settings Conference
          </a>
        </li>
      </ul>
    </li>
    <li>
      <strong>We presented our work on InSight and won first place at the county science fair, were invited to the California Science & Engineering Fair (top 15 projects out of ~350 in Alameda County), and received the “Outstanding AI Innovation” Award.</strong>
    </li>
  </ul>

  <div class="flex flex-wrap justify-center items-center gap-4 mt-6">
    <img src="research_images/insight1.png" alt="InSight Poster 1"
         class="rounded-lg shadow-md border border-gray-200 w-full sm:w-[32%] md:w-[30%]">
    <img src="research_images/insight2.png" alt="InSight Poster 2"
         class="rounded-lg shadow-md border border-gray-200 w-full sm:w-[32%] md:w-[30%]">
    <img src="research_images/insight3.jpg" alt="InSight Poster 3"
         class="rounded-lg shadow-md border border-gray-200 w-full sm:w-[32%] md:w-[30%]">
  </div>
</section>



  <!-- METAFUSION -->
<!-- METAFUSION -->
<section id="metafusion">
  <h2 class="text-2xl font-bold text-[#003366] mb-2">
    MetaFusion: Multimodal Data Fusion Algorithm (Apr 2025)
  </h2>

  <ul class="list-disc list-inside space-y-2">
    <li>
      <strong>We developed MetaFusion, a novel multimodal fusion algorithm to combine clinical data with imaging data.</strong>
      Our method shows superior performance for skin cancer classification (from smartphone images),
      breast cancer detection (from X-ray mammograms), and glaucoma detection (from retinal fundus images).
    </li>
    <li>
      In low-resource settings where medical data are often fragmented or incomplete, data fusion is essential for combining multiple modalities of data — such as imaging and patient metadata — to support accurate diagnosis.
    </li>
    <li>
      Our method is parameter-efficient and demonstrates consistent performance across three diseases.
    </li>
    <li>
      <strong>Our paper on MetaFusion was accepted for presentation at the IEEE International Symposium on Biomedical Imaging (ISBI) 2025.</strong>
    </li>
    <li>
      <strong>Publication:</strong> IEEE Xplore –
      <a href="https://ieeexplore.ieee.org/document/10981175"
         target="_blank"
         class="text-blue-600 hover:underline font-medium">
         MetaFusion: A Novel Method for Integrating Clinical Metadata with Imaging Modalities for Medical Applications
      </a>
    </li>
  </ul>

  <div class="my-6 flex flex-wrap justify-center items-center gap-4">
    <img
      src="research_images/metafusion1.jpg"
      alt="MetaFusion Figure 1"
      class="rounded-lg shadow-md border border-gray-200 w-full sm:w-[48%] md:w-[45%]"
    >
    <img
      src="research_images/metafusion2.png"
      alt="MetaFusion Figure 2"
      class="rounded-lg shadow-md border border-gray-200 w-full sm:w-[48%] md:w-[45%]"
    >
  </div>
</section>


<!-- NASA GENELAB -->
<section id="genelab">
  <h2 class="text-2xl font-bold text-[#003366] mb-2">
    NASA GeneLab Intern – Ananya (July 2024)
  </h2>

  <ul class="list-disc list-inside space-y-2">
    <li>Selected for NASA GeneLab for High Schools, a national bioinformatics and space biology training internship.</li>
    <li>Investigated colon transcriptome data from NASA Rodent Research 6 Mission.</li>
    <li>Applied bioinformatics techniques to study effects of spaceflight on circadian disruption, immune regulation, T-Cell suppression, and downstream impacts on colon inflammation.</li>
    <li><strong>Co-authored a research proposal accepted for presentation at the American Society of Gravitational Space Research Conference.</strong></li>
  </ul>

  <div class="my-4 flex justify-center">
    <img
      src="research_images/genelab.png"
      alt="NASA GeneLab Research Project"
      class="rounded-lg shadow-md border border-gray-200 w-full sm:w-[70%] md:w-[60%]"
    >
  </div>
</section>

   <!-- MEDISEEKER -->
<section id="mediseeker">
  <h2 class="text-2xl font-bold text-[#003366] mb-2">
    Mediseeker: AI-based Drug Discovery Pipeline (2024)
  </h2>

  <ul class="list-disc list-inside space-y-2">
    <li>
      <strong>We develop a three-stage pipeline with AI-based toxicity prediction and personalized drug sensitivity prediction to generate effective and nontoxic drug candidates.</strong>
    </li>
    <li>
      Anti-cancer drug design is expensive and time-consuming, taking almost 2.8 billion dollars and more than 10–17 years to bring a new drug into clinical practice.
    </li>
    <li>
      Personalized drug treatment for cancer has the potential to revolutionize cancer therapy by taking into account tumor response and drug resistance for each individual patient.
    </li>
    <li>
      <strong>Our work on Mediseeker won second place in the county science fair.</strong> 
      We were also <strong>invited as Student Speakers at the Johns Hopkins Global Health Leaders Conference.</strong>
    </li>
  </ul>

  <!-- Image placeholder -->

<div class="my-4 flex flex-wrap justify-center items-center gap-4">
  <img
    src="research_images/drugdiscovery1.jpg"
    alt="Mediseeker Poster 1"
    class="rounded-lg shadow-md border border-gray-200 w-full sm:w-[48%] md:w-[45%]"
  >
  <img
    src="research_images/drugdiscovery2.png"
    alt="Mediseeker Poster 2"
    class="rounded-lg shadow-md border border-gray-200 w-full sm:w-[48%] md:w-[45%]"
  >
</div>

</section>


    <!-- TISSUE OF ORIGIN FOR CUP -->
<!-- TISSUE OF ORIGIN -->
<section id="tissueoforigin">
  <h2 class="text-2xl font-bold text-[#003366] mb-2">
    Deep Learning–Based Identification of Tissue of Origin for Carcinomas of Unknown Primary Using MicroRNA Expression (2023)
  </h2>

  <ul class="list-disc list-inside space-y-2">
    <li>
      <strong>Investigated the potential of microRNAs as biomarkers for detecting the tissue of origin in metastatic cancer using data-driven machine learning approaches.</strong>
    </li>
    <li>
      Carcinoma of Unknown Primary (CUP) is a subset of metastatic cancers in which the primary tumor remains unidentified, accounting for up to 5% of all cancer diagnoses worldwide.
    </li>
    <li>
      The absence of a known primary site complicates treatment decisions and prognosis, underscoring the need for computational models that can infer tissue of origin.
    </li>
    <li>
      Our models achieved up to 97% accuracy in predicting the tissue of origin for metastatic samples, demonstrating feasibility for clinical translation and supporting diagnostic decision-making.
    </li>
    <li>
      <strong>Publication:</strong>
      <a href="https://bioinform.jmir.org/2024/1/e56538"
         target="_blank"
         class="text-blue-600 hover:underline font-medium">
         Journal of Internet Medical Research – Deep Learning–Based Identification of Tissue of Origin for Carcinomas of Unknown Primary Using MicroRNA Expression      </a>
    </li>
  </ul>

  <div class="my-6 flex justify-center">
    <img
      src="research_images/mirna.png"
      alt="Tissue of Origin CUP Project Image"
      class="rounded-lg shadow-md border border-gray-200 w-full sm:w-[70%] md:w-[60%]"
    >
  </div>
</section>



   <!-- LEND ME YOUR EAR -->
<section id="lendmeyourear">
  <h2 class="text-2xl font-bold text-[#003366] mb-2">
    Lend Me Your Ear: AI-based Speech Separation and Enhancement (2023)
  </h2>

  <ul class="list-disc list-inside space-y-2">
    <li>
      <strong>Developed a simple real-time solution to help people hear better when there is interference from other speakers and noise.</strong>
    </li>
    <li>
      Hearing impairment is a serious problem affecting millions of people, making it difficult for them to hear clearly in crowded or noisy environments, even with hearing aids.
    </li>
    <li>
      We developed a new training set that allowed us to train a single model capable of removing interference from both sources, and implemented a real-time version that runs on a laptop with only ~40 ms latency.
    </li>
    <li>
      Improved signal quality by 3 dB for interfering speakers and 7 dB for background noise.
    </li>
    <li>
      <strong>Won first place at the county science fair and received three special awards from company sponsors.</strong>
      Invited to the California State Science Fair and recognized with <strong>Honorable Mention in the state.</strong>
    </li>
  </ul>

  <!-- Image placeholder -->
  <div class="my-4 flex justify-center">
    <img
      src="research_images/lendear.jpg"
      alt="Lend Me Your Ear Project Image"
      class="rounded-lg shadow-md border border-gray-200 w-full sm:w-[70%] md:w-[60%]"
    >
  </div>
</section>

<!-- CONNECT FOUR -->
<section id="connectfour">
  <h2 class="text-2xl font-bold text-[#003366] mb-2">
    AI Agent for Connect Four (2022)
  </h2>

  <ul class="list-disc list-inside space-y-2">
    <li>
      Built an AI-based Connect Four player using tabular Q-learning (reinforcement learning).
    </li>
    <li>
      Compared performance of the model against human players and brute-force search.
    </li>
    <li>
      <strong>Presented our work at the county science fair, winning “Grand Sweepstakes” in the county and overall first place.</strong>
      Invited to Broadcom MASTERS and the California State Science Fair, where it won <strong>third place at the state level.</strong>
    </li>
  </ul>

  <!-- Image placeholder -->

</section>


  </div>

</body>
</html>
